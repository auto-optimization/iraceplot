---
title: "Quick Start"
output: 
  html_document:
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r echo=FALSE, prompt=FALSE, message=FALSE}
library(iraceplot, quietly = TRUE)
load("../data/iraceResults.rda")
```

The configuration process performed by irace will show ar the end of the execution one
or more configurations that are the best performing configurations found. This package
provides a set of functions that allow to further assess the performance of these
configurations and provides support to obtain insights about the details of the configuration
process.

To start, just import the package in the R console: 

```{r eval=FALSE} 
library("iraceplot")
```

The package provided functions to generate plots that display performance-based information
or plots that display parameter valued-based information. 

This guide aims at showing different examples of how the data produced by the irace configurator
can be visualized to obtain information about algorithm performance or high-performing parameters 
settings.

## Executing irace

To use the methods provided by this package you must have an irace data object, 
this object is saved as an Rdata file (irace.Rdata by default) after each irace 
execution. 

During the configuration procedure irace evaluates several candidate configurations 
(parameter settings) on different training insrances, creating an algorithm performance 
data set we call the *training data set*. This information is thus, the data
that irace had access to when configuring the algorithm. 

You can also enable the test evaluation option in irace, in which a set of elite 
configurations will be evaluated on a set of test instances after the execution of 
irace is finished. Nota that this option is not enabled by default and you
must provide the test instances in order to enable it. The performance obtained 
in this evalaution is called the *test data set*. This evaluation helps assess
the results of the configuration in a more "real" setup. For example, we can assess 
if the configuration process incurred in overtuning or if a type of instance
was underrepresented in the training set. We note that irace allows to perform
the test evaluations to the final elite configurations and to the elite configurations
of each iterations. For information about the irace setup we refer you to the irace 
package user guide. 

Note: Before executing irace, consider setting the test evaluation option of irace. 

Once irace is executed, you can load the irace data object in the R console by:

```{r eval=FALSE} 
load("path/to/irace.Rdata")
```

This will load in the console an object called `iraceResults` which contains all data
generated and used by the configurator.

## Visualizing irace data
In the following, we provide an example how this functions could be used to visualize 
the information generated by irace.


### Configurations

Once irace is executed, the first thing you might want to do is to visualize how the
best configurations look like. You can do this with the `parallel_coord` method.

```{r fig.align="center"}
parallel_coord(iraceResults)
```

This plot shows the final elite configurations settings, each line represents one 
configuration. Note that the number of iterations is shown in the scale on the right
and the final elite configurations belong to the last iteration. To visualize 
configurations  considered as elites in each iteration use the `iterations` option:

```{r fig.align="center"}
parallel_coord(iraceResults, iterations=1:iraceResults$state$nbIterations)
```

You can also visualize all configurations sampled in one or more iterations disabling the 
`only_elite` option:

```{r fig.align="center"}
parallel_coord(iraceResults, iterations=c(2,5), only_elite=FALSE)
```

or by using the `parallel_cat` method:

```{r fig.align="center"}
parallel_cat(irace_results = iraceResults, iterations = c(3,5) )
```

### Sampling distributions

In some cases in might be interesting to have a look to the values sampled during the 
configuration procedure as they indicate the areas in the parameter space where irace
detected a high performance.

A general overview of the sampling of one parameter can be obtained with the `sampling_frequency`
function:

```{r fig.align="center"}
 sampling_frequency(iraceResults)
```

A detailed plot showing the sampling by iteration can be obtained with the `sampling_frequency_iteration`
function. This plot shows the convergence of the configuration process reflected in the sampled
parameter values.

```{r fig.align="center"}
sampling_frequency_iteration(iraceResults, param_name = "beta")
```

### Test performance

The test performance of the best final configurations can be visualised using the `boxplot_test`
function. Note that the irace execution include test data (test is not enabled by default).

```{r fig.align="center"}
boxplot_test(iraceResults, type="best")
```

This plot shows all final elite configurations evaluation on the test instance set, we can compare
the performance of these configurations to select one that has the best test performance. 

If the irace data includes iteration elites, its possible to plot the test performance
of elite configurations across the iterations. The best elite configuration on each instance
is displayed in blue:

```{r fig.align="center"}
boxplot_test(iraceResults, type="all")
```

This plot allows to assess the progress of the configuration process regarding the test set performance, 
which would be useful when dealing with heterogeneous instance sets. In these cases, good configurations 
across the full set can be challenging to find and it is possible that the algorithm could be mislead if 
instances sets are prone to introduce bias due to instance ordering. 

Note that in this example the elite configuration with id 808 seems to have a slightly better performance  
than the configuration identified as the best (id:809) by irace. It is important to note that all elite
configurations are not statistically different and thus, its very possible that such situation is observed
when evaluuting test performance, specially when configuring heterogeneous, difficult to balance, instance 
sets. 

To investigate the difference in the performance of two configurations the `scatter_test` function displays
the performance of both configurations paired by instance (each point represents an instance):

```{r fig.align="center"}
scatter_test(iraceResults, id_configurations = c(808,809))
```

### Training performance

To visualize the performance of the final elites observed by irace, the `boxplot_training`
function plots the experiments performed on these configurations. Note that this data corresponds
to the performance generated during the configuration process thus, the number of instances on
which the configurations were evaluated might vary between elite configurations.


```{r fig.align="center"}
boxplot_training(iraceResults)
```


